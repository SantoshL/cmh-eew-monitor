"""
Performance Tracking Module for CMH EEW System
Logs detection metrics and validates against USGS
File: performance_tracker.py
"""

import json
import csv
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import Dict, List, Optional
import requests
import numpy as np

logger = logging.getLogger(__name__)


class PerformanceTracker:
    """Track EEW system performance and generate validation reports"""

    def __init__(self, data_dir: Path):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

    def get_filename(self, data_type: str, date: datetime = None, version: int = 1) -> Path:
        """Generate filename in format: yymmdd-erthqk-type-vX.ext"""
        if date is None:
            date = datetime.now()

        date_str = date.strftime('%y%m%d')
        extensions = {
            'performance': 'csv',
            'validation': 'json',
            'report': 'md'
        }
        ext = extensions.get(data_type, 'txt')
        filename = f"{date_str}-erthqk-{data_type}-v{version}.{ext}"
        return self.data_dir / filename

    def log_detection(self, alert: Dict, usgs_event: Optional[Dict] = None):
        """
        Log detection performance metrics

        Args:
            alert: Your CMH EEW alert
            usgs_event: Corresponding USGS event (ground truth)
        """
        perf_file = self.get_filename('performance')

        # Calculate metrics
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'alert_id': alert.get('alert_id', 'N/A'),
            'cmh_magnitude': alert.get('magnitude', 0),
            'cmh_latitude': alert.get('latitude', 0),
            'cmh_longitude': alert.get('longitude', 0),
            'stations_used': alert.get('stations_used', 0),
            'confidence': alert.get('confidence', 0),
            'detection_time': alert.get('detection_time', 'N/A')
        }

        # Add USGS ground truth if available
        if usgs_event:
            metrics['usgs_event_id'] = usgs_event.get('id', 'N/A')
            metrics['usgs_magnitude'] = usgs_event.get('mag', 0)
            metrics['usgs_latitude'] = usgs_event.get('latitude', 0)
            metrics['usgs_longitude'] = usgs_event.get('longitude', 0)
            metrics['usgs_time'] = usgs_event.get('time', 'N/A')

            # Calculate errors
            metrics['magnitude_error'] = abs(
                metrics['cmh_magnitude'] - metrics['usgs_magnitude']
            )

            metrics['location_error_km'] = self.haversine_distance(
                metrics['cmh_latitude'], metrics['cmh_longitude'],
                metrics['usgs_latitude'], metrics['usgs_longitude']
            )

            # Calculate detection latency
            if alert.get('detection_time') and usgs_event.get('time'):
                try:
                    alert_time = datetime.fromisoformat(
                        alert['detection_time'].replace('Z', '+00:00'))
                    usgs_time = datetime.fromtimestamp(
                        usgs_event['time'] / 1000)
                    metrics['detection_latency_sec'] = (
                        alert_time - usgs_time).total_seconds()
                except:
                    metrics['detection_latency_sec'] = 'N/A'
        else:
            # Potential false positive
            metrics['false_positive'] = True

        # Write to CSV
        file_exists = perf_file.exists()
        with open(perf_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=metrics.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(metrics)

        logger.info(f"ðŸ“Š Performance logged: {alert.get('alert_id', 'N/A')}")

    def haversine_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between two points in km"""
        from math import radians, sin, cos, sqrt, atan2

        R = 6371  # Earth radius in km
        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1

        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
        c = 2 * atan2(sqrt(a), sqrt(1-a))

        return R * c

    def fetch_usgs_earthquakes(self, start_time: datetime, end_time: datetime, min_mag: float = 4.0) -> List[Dict]:
        """Fetch USGS earthquakes for validation"""
        url = "https://earthquake.usgs.gov/fdsnws/event/1/query"
        params = {
            'format': 'geojson',
            'starttime': start_time.isoformat(),
            'endtime': end_time.isoformat(),
            'minmagnitude': min_mag,
            'orderby': 'time'
        }

        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            earthquakes = []
            for feature in data['features']:
                eq = {
                    'id': feature['id'],
                    'time': feature['properties']['time'],
                    'mag': feature['properties']['mag'],
                    'latitude': feature['geometry']['coordinates'][1],
                    'longitude': feature['geometry']['coordinates'][0],
                    'depth': feature['geometry']['coordinates'][2],
                    'place': feature['properties']['place']
                }
                earthquakes.append(eq)

            return earthquakes

        except Exception as e:
            logger.error(f"Error fetching USGS data: {e}")
            return []

    def validate_detections(self, days: int = 1) -> Dict:
        """
        Validate recent detections against USGS ground truth
        Returns comprehensive performance metrics
        """
        perf_file = self.get_filename('performance')

        if not perf_file.exists():
            logger.warning("No performance data found")
            return {}

        # Read performance log
        detections = []
        with open(perf_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                detections.append(row)

        # Filter to recent days
        cutoff = datetime.now() - timedelta(days=days)
        recent_detections = [
            d for d in detections
            if datetime.fromisoformat(d['timestamp']) > cutoff
        ]

        if not recent_detections:
            logger.warning(f"No detections in last {days} days")
            return {}

        # Calculate metrics
        metrics = self._calculate_metrics(recent_detections)

        # Save validation report
        val_file = self.get_filename('validation')
        with open(val_file, 'w') as f:
            json.dump(metrics, f, indent=2)

        logger.info(
            f"âœ“ Validation complete: {len(recent_detections)} detections analyzed")
        return metrics

    def _calculate_metrics(self, detections: List[Dict]) -> Dict:
        """Calculate comprehensive performance metrics"""

        # Filter valid entries
        valid_detections = [
            d for d in detections
            if d.get('usgs_magnitude') and d.get('magnitude_error')
        ]

        false_positives = [
            d for d in detections
            if d.get('false_positive', False)
        ]

        if not valid_detections:
            return {'error': 'No valid detections with USGS ground truth'}

        # Magnitude errors
        mag_errors = [float(d['magnitude_error'])
                      for d in valid_detections if d.get('magnitude_error') != 'N/A']

        # Location errors
        loc_errors = [float(d['location_error_km']) for d in valid_detections if d.get(
            'location_error_km') != 'N/A']

        # Detection latencies
        latencies = [float(d['detection_latency_sec']) for d in valid_detections if d.get(
            'detection_latency_sec') != 'N/A']

        metrics = {
            'period': {
                'total_detections': len(detections),
                'valid_detections': len(valid_detections),
                'false_positives': len(false_positives)
            },
            'magnitude_accuracy': {
                'mean_absolute_error': float(np.mean(mag_errors)) if mag_errors else 0,
                'rmse': float(np.sqrt(np.mean(np.array(mag_errors)**2))) if mag_errors else 0,
                'max_error': float(np.max(mag_errors)) if mag_errors else 0,
                'within_0.3_units': sum(1 for e in mag_errors if e <= 0.3) / len(mag_errors) if mag_errors else 0,
                'within_0.5_units': sum(1 for e in mag_errors if e <= 0.5) / len(mag_errors) if mag_errors else 0
            },
            'location_accuracy': {
                'mean_error_km': float(np.mean(loc_errors)) if loc_errors else 0,
                'median_error_km': float(np.median(loc_errors)) if loc_errors else 0,
                'max_error_km': float(np.max(loc_errors)) if loc_errors else 0,
                'within_10km': sum(1 for e in loc_errors if e <= 10) / len(loc_errors) if loc_errors else 0,
                'within_20km': sum(1 for e in loc_errors if e <= 20) / len(loc_errors) if loc_errors else 0
            },
            'timing': {
                'avg_detection_latency_sec': float(np.mean(latencies)) if latencies else 0,
                'median_latency_sec': float(np.median(latencies)) if latencies else 0,
                'fastest_detection_sec': float(np.min(latencies)) if latencies else 0,
                'slowest_detection_sec': float(np.max(latencies)) if latencies else 0
            },
            'reliability': {
                'false_positive_rate': len(false_positives) / len(detections) if detections else 0,
                'detection_success_rate': len(valid_detections) / len(detections) if detections else 0
            },
            'shakealert_comparison': self._compare_to_shakealert(
                mag_errors,
                latencies,
                len(false_positives) / len(detections) if detections else 0
            )
        }

        return metrics

    def _compare_to_shakealert(self, mag_errors: List[float], latencies: List[float], fp_rate: float) -> Dict:
        """Compare performance to ShakeAlert benchmarks"""

        shakealert_benchmarks = {
            'magnitude_accuracy': 0.5,
            'avg_latency': 3.5,
            'false_positive_rate': 0.10
        }

        cmh_mag_accuracy = float(np.mean(mag_errors)) if mag_errors else 0
        cmh_latency = float(np.mean(latencies)) if latencies else 0

        return {
            'magnitude_accuracy': {
                'cmh': round(cmh_mag_accuracy, 2),
                'shakealert': shakealert_benchmarks['magnitude_accuracy'],
                'performance': 'Better' if cmh_mag_accuracy < shakealert_benchmarks['magnitude_accuracy'] else 'Comparable'
            },
            'detection_speed': {
                'cmh': round(cmh_latency, 2),
                'shakealert': shakealert_benchmarks['avg_latency'],
                'performance': 'Better' if cmh_latency < shakealert_benchmarks['avg_latency'] else 'Comparable'
            },
            'false_positives': {
                'cmh': round(fp_rate * 100, 1),
                'shakealert': round(shakealert_benchmarks['false_positive_rate'] * 100, 1),
                'performance': 'Better' if fp_rate < shakealert_benchmarks['false_positive_rate'] else 'Needs Improvement'
            }
        }


# Test code
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    tracker = PerformanceTracker(Path('./data'))

    # Example: log a detection
    sample_alert = {
        'alert_id': 'CMH_20251202_201500',
        'magnitude': 5.2,
        'latitude': 35.5,
        'longitude': 138.5,
        'stations_used': 5,
        'confidence': 0.85,
        'detection_time': datetime.now().isoformat()
    }

    sample_usgs = {
        'id': 'us7000test',
        'mag': 5.3,
        'latitude': 35.52,
        'longitude': 138.48,
        'time': int(datetime.now().timestamp() * 1000),
        'place': 'Japan'
    }

    tracker.log_detection(sample_alert, sample_usgs)
    print("âœ“ Performance tracking test passed")
